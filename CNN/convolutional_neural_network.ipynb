{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0-rc0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this tutorial on [image data augmentation](https://stepup.ai/exploring_data_augmentation_keras/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set\n",
    "\n",
    "[keras image preprocessing API](https://keras.io/api/preprocessing/image/)\n",
    " \n",
    "We use the [ImageDataGenerator](https://keras.io/api/preprocessing/image/#imagedatagenerator-class) which can generate batches of data with real time transformations, as defined in the construction of the generator object.  \n",
    "\n",
    "The rescale parameter set to this specific value (1/255) provides [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling): normalization of the input.  \n",
    "Since each pixel's value is in the range [0 - 255], dividing by 255 will map the values to the [0 - 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "# target_size: the size of the images when loaded\n",
    "# here we resize the images to 64x64 pixels\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set\n",
    "\n",
    "When loading the test set, we don't perform any transformations, since we want to predict the class of the actual images. (That's why they're called test images. It wouldn't make much sense trying to predict the classes of different images, which is what we obtain after the augmentation process).  \n",
    "\n",
    "It is crucial, however, to perform feature scaling, since we've done so when we processed the training images.  \n",
    "Skipping this step would result in trying to predict image classes for images that are entirely on a different value scale, and the accuracy would be, as a result, very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# class_mode: binary/categorical\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution\n",
    "\n",
    "`filters`: the number of feature detectors (a.k.a. kernels / filters) you want to apply to the input.  \n",
    "`input_shape = [64, 64, 3]`: 64x64 pixels, 3 channels (RGB). This parameter is only needed for the **first layer**. \n",
    "                             Additional convolutional layers can infer the size from the layer connected to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection\n",
    "\n",
    "`units`: the number of neurons in the layer connected directly to the output of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer\n",
    "\n",
    "`activation`: the activation function.\n",
    "For binary classification, use sigmoid.  \n",
    "For categorical classification, use softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 60s 239ms/step - loss: 0.7126 - accuracy: 0.5262 - val_loss: 0.6328 - val_accuracy: 0.6230\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 63s 251ms/step - loss: 0.6364 - accuracy: 0.6313 - val_loss: 0.6072 - val_accuracy: 0.6680\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.5891 - accuracy: 0.6868 - val_loss: 0.5765 - val_accuracy: 0.6965\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 66s 266ms/step - loss: 0.5440 - accuracy: 0.7235 - val_loss: 0.5910 - val_accuracy: 0.7080\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 67s 268ms/step - loss: 0.5227 - accuracy: 0.7433 - val_loss: 0.5072 - val_accuracy: 0.7475\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 67s 267ms/step - loss: 0.4870 - accuracy: 0.7708 - val_loss: 0.4891 - val_accuracy: 0.7595\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 67s 270ms/step - loss: 0.4714 - accuracy: 0.7695 - val_loss: 0.4717 - val_accuracy: 0.7680\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 66s 263ms/step - loss: 0.4614 - accuracy: 0.7813 - val_loss: 0.4630 - val_accuracy: 0.7780\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 67s 266ms/step - loss: 0.4539 - accuracy: 0.7777 - val_loss: 0.4717 - val_accuracy: 0.7775\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 68s 272ms/step - loss: 0.4380 - accuracy: 0.7970 - val_loss: 0.5607 - val_accuracy: 0.7340\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 69s 278ms/step - loss: 0.4282 - accuracy: 0.7944 - val_loss: 0.5381 - val_accuracy: 0.7465\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 69s 274ms/step - loss: 0.4095 - accuracy: 0.8048 - val_loss: 0.5677 - val_accuracy: 0.7485\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 0.4055 - accuracy: 0.8112 - val_loss: 0.4874 - val_accuracy: 0.7830\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 65s 261ms/step - loss: 0.3955 - accuracy: 0.8140 - val_loss: 0.4930 - val_accuracy: 0.7830\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 60s 242ms/step - loss: 0.3753 - accuracy: 0.8281 - val_loss: 0.4663 - val_accuracy: 0.7935\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 0.3549 - accuracy: 0.8364 - val_loss: 0.4554 - val_accuracy: 0.7905\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3588 - accuracy: 0.8338 - val_loss: 0.5070 - val_accuracy: 0.7875\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.3170 - accuracy: 0.8571 - val_loss: 0.4517 - val_accuracy: 0.7865\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3241 - accuracy: 0.8556 - val_loss: 0.4631 - val_accuracy: 0.8055\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3098 - accuracy: 0.8645 - val_loss: 0.4928 - val_accuracy: 0.7985\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.2982 - accuracy: 0.8713 - val_loss: 0.4830 - val_accuracy: 0.7845\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.2887 - accuracy: 0.8834 - val_loss: 0.5751 - val_accuracy: 0.7700\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.2825 - accuracy: 0.8773 - val_loss: 0.5208 - val_accuracy: 0.7975\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.2669 - accuracy: 0.8880 - val_loss: 0.6315 - val_accuracy: 0.7640\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.2594 - accuracy: 0.8887 - val_loss: 0.5915 - val_accuracy: 0.7740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff0f8b999d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction\n",
    "\n",
    "**Note:**  \n",
    "`expand_dims` is needed to add an extra dimension to the input image, since we've trained the model in batch mode.  Meaning, the first dimension of the input is the batch axis.  \n",
    "In this case, we're predicting the label of a single image, so the first dimension has only one call. (whereas during training, the first dimension has 32 cell, or more generally, `batch_size` cells)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `class_indices` method returns the index assignment for each class (sometimes referred to as categore or label).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image/255.0) # normalize the image. This is necessary because training was done on normalized input.\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] > 0.5:\n",
    "  prediction = 'cat'\n",
    "else:\n",
    "  prediction = 'dog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the `> 0.5` ?  \n",
    "The [tf.kera.Model.predict](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict) method returns an array of predictions.  \n",
    "The size of the array matches the number of categories (in this example it's 2).  \n",
    "Each cell contains the **prediction confidence**; how sure the model is that the input belongs to a certain category.  \n",
    "\n",
    "For example, a prediction might look like this: `[0.95, 0.05]`, meaning the model is 95% sure that the image is of a cat (index 0 corresponds to the label 'cat' according to `class_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED9KB3I54c1i"
   },
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
